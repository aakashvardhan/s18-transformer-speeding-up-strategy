{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+q+gXmjqfylIt6VgNWR46"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_asPa0_JonFC","executionInfo":{"status":"ok","timestamp":1718535832435,"user_tz":-240,"elapsed":570,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["import os\n","local_dir = 's18-transformer-speeding-up-strategy'\n","repo_url = 'https://github.com/aakashvardhan/s18-transformer-speeding-up-strategy.git'\n","\n","# Check if the local directory already exists\n","if not os.path.exists(local_dir):\n","    # Clone the repository because it does not exist\n","    !git clone --quiet {repo_url}\n","else:\n","    # Change directory to the local repository\n","    %cd {local_dir}\n","    # Pull the latest changes because the repository already exists\n","    !git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HJBp9Hho4Vk","executionInfo":{"status":"ok","timestamp":1718535833325,"user_tz":-240,"elapsed":390,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"4faa5a89-b544-4a89-ee28-42e74b86b545"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/s18-transformer-speeding-up-strategy\n","Already up to date.\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/s18-transformer-speeding-up-strategy')"],"metadata":{"id":"qoY7Ema_o6T_","executionInfo":{"status":"ok","timestamp":1718535833325,"user_tz":-240,"elapsed":1,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install -q -r /content/s18-transformer-speeding-up-strategy/requirements.txt"],"metadata":{"id":"I0KqQOVao8LD","executionInfo":{"status":"ok","timestamp":1718535847393,"user_tz":-240,"elapsed":14069,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from lightning import LightningDataModule\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import WordLevelTrainer\n","from torch.nn.utils.rnn import pad_sequence\n","from pathlib import Path\n","\n","\n","class BillingualDataset(Dataset):\n","    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.ds = ds\n","        self.tokenizer_src = tokenizer_src\n","        self.tokenizer_tgt = tokenizer_tgt\n","        self.src_lang = src_lang\n","        self.tgt_lang = tgt_lang\n","\n","        self.sos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n","        )\n","        self.eos_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n","        )\n","        self.pad_token = torch.tensor(\n","            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n","        )\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        src_tgt_pair = self.ds[idx]\n","        src_text = src_tgt_pair[\"translation\"][self.src_lang]\n","        tgt_text = src_tgt_pair[\"translation\"][self.tgt_lang]\n","\n","        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n","        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n","\n","        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n","        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n","        # For encoding, we PAD both SOS and EOS. For decoding, we only pad SOS.\n","        # THe model is required to predict EOS and stop on its own.\n","\n","        # Make sure that padding is not negative (ie the sentance is too long)\n","        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n","            raise ValueError(\"Sentence too long\")\n","\n","        encoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(enc_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor(\n","                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64\n","                ),\n","            ],\n","            dim=0,\n","        )\n","\n","        decoder_input = torch.cat(\n","            [\n","                self.sos_token,\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                torch.tensor(\n","                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n","                ),\n","            ],\n","            dim=0,\n","        )\n","\n","        label = torch.cat(\n","            [\n","                torch.tensor(dec_input_tokens, dtype=torch.int64),\n","                self.eos_token,\n","                torch.tensor(\n","                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n","                ),\n","            ],\n","            dim=0,\n","        )\n","\n","        assert encoder_input.size(0) == self.seq_len\n","        assert decoder_input.size(0) == self.seq_len\n","        assert label.size(0) == self.seq_len\n","\n","        return {\n","            \"encoder_input\": encoder_input,\n","            \"decoder_input\": decoder_input,\n","            # \"encoder_mask\": (encoder_input != self.pad_token)\n","            # .unsqueeze(0)\n","            # .unsqueeze(0)\n","            # .int(),\n","            # # encoder mask: (1, 1, seq_len) -> Has 1 when there is text and 0 when there is pad (no text)\n","            # \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int()\n","            # & causal_mask(decoder_input.size(0)),\n","            # (1, seq_len) and (1, seq_len, seq_len)\n","            # Will get 0 for all pads. And 0 for earlier text.\n","            \"label\": label,\n","            \"src_text\": src_text,\n","            \"tgt_text\": tgt_text,\n","            \"encoder_str_length\": len(enc_input_tokens),\n","            \"decoder_str_length\": len(dec_input_tokens),\n","        }\n","\n","    def has_nan(self):\n","        for idx in range(len(self)):\n","            sample = self[idx]\n","            for key, value in sample.items():\n","                if isinstance(value, torch.Tensor) and torch.isnan(value).any():\n","                    print(f\"NaN found in sample {idx}, key: {key}\")\n","                    return True\n","        return False"],"metadata":{"id":"5nJxaQ8ypAL8","executionInfo":{"status":"ok","timestamp":1718535856618,"user_tz":-240,"elapsed":9227,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# run tokenizer\n","def get_all_sentences(ds, lang):\n","    for item in ds:\n","        yield item['translation'][lang]\n","\n","def get_or_build_tokenizer(config, ds,lang):\n","\n","    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n","    if not Path.exists(tokenizer_path):\n","        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n","        tokenizer.pre_tokenizer = Whitespace()\n","        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"],min_frequency = 2)\n","        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer = trainer)\n","        tokenizer.save(str(tokenizer_path))\n","    else:\n","        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n","    return tokenizer"],"metadata":{"id":"wPwZJBKWqpJe","executionInfo":{"status":"ok","timestamp":1718535856619,"user_tz":-240,"elapsed":4,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from utils import clean_long_text\n","from config_file import get_config\n","config = get_config()\n","\n","\n","ds_raw = load_dataset('opus_books',f\"{config['lang_src']}-{config['lang_tgt']}\",split='train' )\n","ds_raw = clean_long_text(config,ds_raw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJIsziUFqGUC","executionInfo":{"status":"ok","timestamp":1718535865741,"user_tz":-240,"elapsed":9125,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"a69b278c-0342-42e1-83f0-5b1ed6aa069d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n","tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])"],"metadata":{"id":"FFPleuSHrMPo","executionInfo":{"status":"ok","timestamp":1718535865741,"user_tz":-240,"elapsed":13,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tokenizer_src"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIhhS2wMrdTO","executionInfo":{"status":"ok","timestamp":1718535865741,"user_tz":-240,"elapsed":13,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"a976ac32-51bd-4652-c2d0-3a8363c57f32"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tokenizers.Tokenizer at 0x7c4d55661c30>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train_ds = BillingualDataset(ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])"],"metadata":{"id":"0s_CnmYjrwku","executionInfo":{"status":"ok","timestamp":1718535865741,"user_tz":-240,"elapsed":12,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_ds.__getitem__(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zO-bvSpir5nK","executionInfo":{"status":"ok","timestamp":1718535865741,"user_tz":-240,"elapsed":12,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"62237a82-3627-4164-8b39-3a33a685d7dc"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'encoder_input': tensor([  2, 114,  32,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n","           1,   1,   1,   1,   1,   1]),\n"," 'decoder_input': tensor([2, 0, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n"," 'label': tensor([0, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n"," 'src_text': 'No!',\n"," 'tgt_text': 'DÃ glielo.',\n"," 'encoder_str_length': 2,\n"," 'decoder_str_length': 2}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["if train_ds.has_nan():\n","    print(\"NaN values found.\")\n","else:\n","    print(\"No NaN values found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EZtv50k8sCvE","executionInfo":{"status":"ok","timestamp":1718535875848,"user_tz":-240,"elapsed":10118,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"3ca5a775-d12d-4220-d92e-f19fb748efe7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["No NaN values found.\n"]}]},{"cell_type":"code","source":["from dataset import LiTDataModule\n","\n","# Initialize and setup the LightningDataModule\n","data_module = LiTDataModule(config)\n","data_module.setup()\n","\n","# Get train DataLoader and iterate over it to find NaN values\n","train_dl = data_module.train_dataloader()\n","\n","for batch_idx, batch in enumerate(train_dl):\n","    nan_found = False\n","\n","    # Check for NaN values in each tensor within the batch\n","    for key, tensor in batch.items():\n","        if isinstance(tensor, torch.Tensor) and torch.isnan(tensor).any():\n","            print(f\"NaN found in batch {batch_idx}, key: {key}\")\n","            nan_found = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f93Up8nKu03g","executionInfo":{"status":"ok","timestamp":1718536379983,"user_tz":-240,"elapsed":22000,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"52fdc8f3-9b41-4241-9143-1bbd3fe8d2ab"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Max length of the source sentence : 45\n","Max length of the source target : 41\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"URHPjXsyv8Yu"},"execution_count":null,"outputs":[]}]}